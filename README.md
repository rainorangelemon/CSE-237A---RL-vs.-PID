# Balancing Bot - PID versus Reinforcement Learning

Chenning Yu (solo project)

## Summary

This is the final project done by myself during study at UCSD. Grab some coffee and watch [an amazing video here](https://youtu.be/C5C9cFQyAiM)!

[A report with more details](report.pdf) is also provided!

## Motivation

Currently, the RL algorithm for inverted pendulum problem is quite mature, which is exactly equivalent to controlling a balance bot. However, traditionally these algorithms are only trained in a simulated environment, and such environment lacks the ability of dealing with physical reality, which has signals with noise.

On the other side, although there are already a dozen of demos on the web showing how to build a Segway based on Arduino or Raspberry Pi, these demos are only played in a fixed landscape, which contains constant friction coefficient. As a result, the control algorithm might suffer from generalizing to other environments, which has a totally different friction constant. An RL-based learning algorithm can solve this issue, because it can learn new parameters from trail-and-errors in new environments.

As a result, I would love to combine these two areas into one unified framework, to make RL method robust, and to make control algorithm generalized.


## Related Work
### Previous Works

The original Q-learning paper comes around [1995](https://link.springer.com/content/pdf/10.1007/BF00992698.pdf), where it introduces the Q function in order to decouple the system transition function from the policy in the value evaluation scenario. Later around [2013](https://arxiv.org/pdf/1312.5602.pdf), people starts using neural network as a approximation heuristic to evaluate the Q-value in more complex environments. This approach starts the trend of using neural networks in reinforcement learning areas.

Although there are many four-wheel, there is no self-balancing commercial product based on RPi. On the other side, there are still a lot of demo on [youtube](https://www.youtube.com/watch?v=D0ydpIZFtuM) showing hand-maded balancing bot.

### My Uniqueness

From embedded system software perspective, instead of hard coding the parameter as what PID did, the controlling algorithm has the ability to learn the environment by itself using RL methods. From RL side, this is be an excellent platform to deploy RL method in reality, since the algorithm has to deal with noise, which cannot really be simulated based on environment generated by computers.

